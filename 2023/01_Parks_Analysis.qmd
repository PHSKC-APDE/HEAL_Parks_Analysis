---
title: "HEAL Parks 2023 Analysis"
author: "Ronald Buie"
date-modified: "`r Sys.Date()`"

format: 
  pdf:
    toc: true
    number-sections: true
    df-print: kable
    fig-height: 12
    fig-width: 16
execute: 
  echo: false
  warning: false
  output: false
---

# Front Matter

This document outlines procedures, technical considerations, and analytic results for the 2023 analysis of data from the HEAL Parks Study. The primary purpose of this PDF is technical review by analyst and project managers to confirm the process and data quality. 

For general information about the project please review the git (make into link) or contact Seth Schromen-Wawrin (email link)

## Major Inputs

Inputs to this script are contained at ./inputs/*.* and include

* Access to the ITHS REDCap(link) project: "Public Health - Seattle & King County Park Observations"
 + or data extracted from that project
* file of park meta data, including address, zip, city, neighborhood, official name, and REDCap name for each park
* (optional) file containing record ids to redact

## Major Outputs

Outputs to this script are sent to ./outputs/<output-type>/*.*

Types include

* data-metadata - tables of analyzable line item data from different stages of preparation leading up to analysis, generally in csv format
* tables - tabular outputs of analysis, generally in xlsx format
* charts - chart outputs of analysis, generally in pdf and/or png format

```{r create target directories}

dir.create("./outputs/charts", recursive = TRUE, showWarnings = FALSE)
dir.create("./outputs/tables", recursive = TRUE, showWarnings = FALSE)
dir.create("./outputs/data-metadata", recursive = TRUE, showWarnings = FALSE)
```

## This is a Quarto Generated Document

By rendering/knitting the qmd file, the analysis is re-executed, this document rebuilt, and new outputs are generated. To learn more about Quarto see <https://quarto.org>.

# Setup & Environment

This script was last executed using `r version$version.string`.

```{r define constant and environment variables}
SECRETS_LOCALE <- "./2023/inputs/secrets.txt" #REDCap project API access information goes here
DATA_LOCALE <- "2023/inputs/data.csv" #use this if REDCap is not accessable

USEGOODDATA <- TRUE #flag to only use data that passess QA in the report
OBSINDAYTOLERANCE <- 3 #a data improvement parameter...
CHARTWIDTH <- 3.5 #width for charts to generate for reproduciton
CHARTHEIGHT <- 2.3 #height of charts to generate for reproduction
```

## Packages used

```{r load required packages}
# First check if pacman is installed. If not, install it.
if(!"pacman" %in% installed.packages()) {
  install.packages("pacman")
}

# load and install packages using pacman. 
# Pacman will install and load missing packages.
pacman::p_load(openxlsx2, #read xlsx files
               ggplot2, #used to create pretty charts
               here,  #used to simplify calls to local directory
               RCurl, #used to prepare and parse json/API querries
               jsonlite, #used to parse json results
               data.table, #data structure and analytics framework used throughout
               kableExtra) #kable extension used to generate pretty output


#configure any package specific options here
options(knitr.table.format = "latex") 

```

`r installed.packages()[names(sessionInfo()$otherPkgs), "Version"]`

## Secrets and tokens

In order to pull data directly from REDCap, API information must be provided. You should create a file called "secrets.txt" in this directory. This file should contain the following information:

Note, that the .gitignore for this project is configured to exclude your secrets.txt file, so, by default, it will not upload to github, and you will not be able to see other users' secrets.txt. They are only stored on your machine.

# Data Preperation

## Extraction

```{r import data}

if (file.exists(here(SECRETS_LOCALE))) {
  source(here("2023/inputs/secrets.txt"))
} else {
  warning("secrets not found. checking for data.csv")
  if(!file.exists(here(DATA_LOCALE))){
    stop("data not found")
  }
}


result <- postForm(
    api_url,
    token=api_token,
    content='record',
    format='json',
    type='flat',
    csvDelimiter='',
    rawOrLabel='label',
    rawOrLabelHeaders='label',
    exportCheckboxLabel='true',
    exportSurveyFields='true',
    exportDataAccessGroups='false',
    returnFormat='json',
    dateRangeBegin='2023-07-01 01:01:01'
)

RawSOPARCObservations <- as.data.table(fromJSON(result), )

write.csv(RawSOPARCObservations,here("2023/outputs/data-metadata/REDCapSOPARCObservations.csv"), row.names = FALSE)

```


## Transformation

```{r convert object and set data types}


#convert countables to numeric
RawSOPARCObservations$tar_area <- as.numeric(RawSOPARCObservations$tar_area)
RawSOPARCObservations$num_child_prim <- as.numeric(RawSOPARCObservations$num_child_prim)
RawSOPARCObservations$num_child_snd <- as.numeric(RawSOPARCObservations$num_child_snd)
RawSOPARCObservations$num_child_spec <- as.numeric(RawSOPARCObservations$num_child_spec)
RawSOPARCObservations$num_teen_prim <- as.numeric(RawSOPARCObservations$num_teen_prim)
RawSOPARCObservations$num_teen_snd <- as.numeric(RawSOPARCObservations$num_teen_snd)
RawSOPARCObservations$num_teen_spec <- as.numeric(RawSOPARCObservations$num_teen_spec)
RawSOPARCObservations$num_adult_prim <- as.numeric(RawSOPARCObservations$num_adult_prim)
RawSOPARCObservations$num_adult_snd <- as.numeric(RawSOPARCObservations$num_adult_snd)
RawSOPARCObservations$num_adult_spec <- as.numeric(RawSOPARCObservations$num_adult_spec)
RawSOPARCObservations$num_senior_prim <- as.numeric(RawSOPARCObservations$num_senior_prim)
RawSOPARCObservations$num_senior_snd <- as.numeric(RawSOPARCObservations$num_senior_snd)
RawSOPARCObservations$num_senior_spec <- as.numeric(RawSOPARCObservations$num_senior_spec)

#NOT NEEDED #properly type all text fields and turn "" to NA
# chcols = names(RawSOPARCObservations)[sapply(RawSOPARCObservations, is.character)]
# RawSOPARCObservations[, (chcols) := lapply(.SD, type.convert, as.is=TRUE), .SDcols=chcols]




#some saved work
                #accessible_check <- SOPARCObservations$accessible
                # usable_check <- SOPARCObservations$usable
                # lit_check <- SOPARCObservations$lit
                # occupied_check <- SOPARCObservations$occupied
                # supervised_check <- SOPARCObservations$supervised
                # organized_check <- SOPARCObservations$organized
                # equipped_check <- SOPARCObservations$equipped
                # num_child_prim_check <- SOPARCObservations$num_child_prim
                # num_child_snd_check <- SOPARCObservations$num_child_snd
                # num_child_spec_check <- SOPARCObservations$num_child_spec
                # num_teen_prim_check <- SOPARCObservations$num_teen_prim
                # num_teen_snd_check <- SOPARCObservations$num_teen_snd
                # num_teen_spec_check <- SOPARCObservations$num_teen_spec
                # num_adult_prim_check <- SOPARCObservations$num_adult_prim
                # num_adult_snd_check <- SOPARCObservations$num_adult_snd
                # num_adult_spec_check <- SOPARCObservations$num_adult_spec
                # num_senior_prim_check <- SOPARCObservations$num_senior_prim
                # num_senior_snd_check <- SOPARCObservations$num_senior_snd
                # num_senior_spec_check <- SOPARCObservations$num_senior_spec
                # prim_act_name_check <- SOPARCObservations$prim_act_name
                # prim_act_other_check <- SOPARCObservations$prim_act_other
                # yesno_snd_check <- SOPARCObservations$yesno_snd
                # snd_act_name_check <- SOPARCObservations$snd_act_name
                # other_act_snd_check <- SOPARCObservations$other_act_snd
                # yesno_spec_check <- SOPARCObservations$yesno_spec
                # spec_act_name_check <- SOPARCObservations$spec_act_name
                # other_act_spec_check <- SOPARCObservations$other_act_spec
                # comments_check <- SOPARCObservations$comments


```




```{r add timestamp information and metadata}
#convert time to time data type and save as separate variables
RawSOPARCObservations[, timestampPOSIX := as.POSIXct(strptime(start_time, "%Y-%m-%d %H:%M"), "America/Los_Angeles") ] #a POSIX format of time
RawSOPARCObservations[, datePOSIX := as.Date(RawSOPARCObservations$timestampPOSIX, tz = "America/Los_Angeles")] #a data.table integer format of second of the day
#RawSOPARCObservations[, idate := IDateTime(timestampPOSIX)$idate] #a data.table integer format of second of the day
#add day of the week
RawSOPARCObservations[, day := weekdays(timestampPOSIX)]
#add month indicator
RawSOPARCObservations[, month := months(timestampPOSIX)]
#add weekend indicator
RawSOPARCObservations[, weekend := ifelse(day %in% c("Saturday", "Sunday"), 1, 0)]

```




## Cleaning

| Description                                  | Details                                                                                                                                                                                       |
|-----------------------------------------|-------------------------------|
| Drop pre-study data                          | Study start date is 7/1/23 with the first park being Garfield Playfield. Observations prior to this date or the first observation of Garfield are dropped                                     |
| Drop incomplete entries                      | observations where the REDCap status is not "Complete", that are missing a timestamp, or missing a park name are dropped                                                                      |
| Drop duplicates                              | duplicate entries (exclusive of redcapID) are dropped. Note that where this creates incomplete days, this is corrected in later QA                                                            |
| Drop inacurate subareas                      | Some parks were identified as having having sub area data input without having multiple sub areas (and so shouldn't be processed as sub areas). The subarea entry for these parks is removed. |
| Drop observations identified by human review | Some observations were identified through review of base data by program managers. These are identified in the file "Records-to-Remove.xlsx"                                                  |

```{r identify and drop incorrect observations}

SOPARCObservations <- copy(RawSOPARCObservations)

#remove duplicates
#SOPARCObservations <- SOPARCObservations[!duplicated(SOPARCObservations[, setdiff(names(SOPARCObservations), "record_id"), with = FALSE]),]

####TESTING####
#remove data with incomplete entry status
SOPARCObservations <- SOPARCObservations[park_scan_data_collection_complete %in% "Complete",]
###############

#drop observations without a time, we cannot use these
SOPARCObservations <- SOPARCObservations[start_time != "",]
#drop observations with missing park names, we cannot use these
SOPARCObservations <- SOPARCObservations[!(park_name %in% ""),]

#redact inaccurate subarea labels
SOPARCObservations[park_name %in% c("Hazelnut Park", 
                                    "Puget Sound Park",
                                    "Tukwila Community Center",
                                    "Tukwila Park",
                                    "Fort Dent Park - South",
                                    "Fort Dent Park - North") & sub_area == "0", sub_area := ""]

#remove pre-study observation (test data)
SOPARCObservations <- SOPARCObservations[record_id >= 141,]
SOPARCObservations <- SOPARCObservations[datePOSIX >= "2023-07-01",]

#remove extra day from Manhattan park
#Manhattan Park has one too many days collected. We drop an extra day here. 
# unique(SOPARCObservations[park_name %in% "Manhattan Park" , datePOSIX]) #check dates collected to ocnfirm where they are in the calendar
# unique(SOPARCObservations[park_name %in% "Manhattan Park" & day == "Friday", datePOSIX]) #check days of the week of each: two are friday
# unique(SOPARCObservations[park_name %in% "Manhattan Park" , .(.N,first(obs_initial)),by =datePOSIX]) #double check that all days are complete/identical in number of observations
#after review, we will drop friday 18/08/2023 because it is furthers from the other dates. 
SOPARCObservations <- SOPARCObservations[!(park_name %in% "Manhattan Park" & datePOSIX == "2023-08-18"),]

#remove identified records
RemoveMe <- openxlsx2::read_xlsx(here("2023/inputs/Records-to-Remove.xlsx"))
SOPARCObservations <- SOPARCObservations[!(record_id %in% RemoveMe$record_id),]

#reorder given new data
setorder(SOPARCObservations, cols = "timestampPOSIX")



```

## Collapse sub areas

For this analysis, we are not using sub areas. These can be collapsed into single areas. For each observation, sub areas will be collapsed using the following rules: \* numerical observations will be added together \* categorical observations become affirmative/existing if any of the subareas are affirmative/existing \* timestamp of the earliest observation in the set will be used

Sub areas of the same target area will be assumed to be of the same observation period based on the following logic: \* for a sequence of sub areas observed in the same 50 hours period apparently missing sub-areas will be ignored (assumption: not all sub areas are necessary)

If non unique sub area labels are identified: \* check if the expected list of sub areas are in teh data set if too many, attempt to identify redundancies and remove these if too few, interpolate missing information

```{r collapse sub areas into a single area}
SOPARCObservationsSubAreas <- copy(SOPARCObservations[sub_area != "",])

remove(SOPARCCollapsedSubAreas) #if this DT exist, remove it because we will recreate it in this chunk

listToCollapse <- SOPARCObservationsSubAreas[ ,  .(sub_area = unique(sub_area)) , by = c("park_name", "tar_area")]
listToCollapseStarts <- SOPARCObservationsSubAreas[, .(first_sub_area = first(sub_area)), by = c("park_name", "tar_area")]



for(index in 1:nrow(listToCollapseStarts)) {
  park <- listToCollapseStarts[index,]$park_name
  target_area <- listToCollapseStarts[index,]$tar_area
  first_sub_area_here <- listToCollapseStarts[index,]$first_sub_area
  SubAreaStarts <- SOPARCObservationsSubAreas[park_name == park & tar_area == target_area & sub_area == first_sub_area_here,]
  #first_sub_area_list <- SOPARCObservationsSubAreas[park_name == park & tar_area == target_area & sub_area == ]
  for(collapseIndex in 1:nrow(SubAreaStarts)) {
    date <- SubAreaStarts[collapseIndex, datePOSIX]
    starttime <- SubAreaStarts[collapseIndex, timestampPOSIX]
    
    endtime <- SubAreaStarts[collapseIndex+1,timestampPOSIX-1]
    if(is.na(endtime)) { endtime <- starttime + 450}
    maxExpectedObservations <- length(unique(SOPARCObservationsSubAreas[park_name == park & tar_area == target_area & datePOSIX == date & (timestampPOSIX >= starttime & timestampPOSIX < endtime ), sub_area]))
    if(nrow(SOPARCObservationsSubAreas[park_name == park & tar_area == target_area & datePOSIX == date & (timestampPOSIX >= starttime & timestampPOSIX < endtime ), ]) <= maxExpectedObservations ){
    TempCollapseResult <- SOPARCObservationsSubAreas[park_name == park & tar_area == target_area & datePOSIX == date & (timestampPOSIX >= starttime & timestampPOSIX < endtime ), .(record_id = first(record_id), 
                                                                                                                                                    redcap_survey_identifier = first(redcap_survey_identifier),
                                                                                                                                                    park_scan_data_collection_timestamp = first(park_scan_data_collection_timestamp),
                                                                                                                                                    obs_initial = first(obs_initial),
                                                                                                                                                    start_time = first(start_time),
                                                                                                                                                    park_name = first(park_name),
                                                                                                                                                    tar_area = first(tar_area),
                                                                                                                                                    sub_area = "X",
                                                                                                                                                    accessible = ifelse(any(accessible %in% "Yes"), "Yes", "No"),
                                                                                                                                                    usable = ifelse(any(usable %in% "Yes"), "Yes", "No"),
                                                                                                                                                    lit = ifelse(any(lit %in% "Yes"), "Yes", "No"),
                                                                                                                                                    occupied = ifelse(any(occupied %in% "Yes"), "Yes", "No"),
                                                                                                                                                    supervised = ifelse(any(supervised %in% "Yes"), "Yes", "No"),
                                                                                                                                                    organized = ifelse(any(organized %in% "Yes"), "Yes", "No"),
                                                                                                                                                    equipped = ifelse(any(equipped %in% "Yes"), "Yes", "No"),
                                                                                                                                                    num_child_prim = sum(num_child_prim, na.rm = TRUE),
                                                                                                                                                    num_child_snd = sum(num_child_snd, na.rm = TRUE),
                                                                                                                                                    num_child_spec = sum(num_child_spec, na.rm = TRUE),
                                                                                                                                                    num_teen_prim = sum(num_teen_prim, na.rm = TRUE),
                                                                                                                                                    num_teen_snd = sum(num_teen_snd, na.rm = TRUE),
                                                                                                                                                    num_teen_spec = sum(num_teen_spec, na.rm = TRUE),
                                                                                                                                                    num_adult_prim = sum(num_adult_prim, na.rm = TRUE),
                                                                                                                                                    num_adult_snd = sum(num_adult_snd, na.rm = TRUE),
                                                                                                                                                    num_adult_spec = sum(num_adult_spec, na.rm = TRUE),
                                                                                                                                                    num_senior_prim = sum(num_senior_prim, na.rm = TRUE),
                                                                                                                                                    num_senior_snd = sum(num_senior_snd, na.rm = TRUE),
                                                                                                                                                    num_senior_spec = sum(num_senior_spec, na.rm = TRUE),
                                                                                                                                                    prim_act_name = gsub("^(\\;)*|(?<=(\\;))(\\;)|(\\;)*$","",paste(unique(prim_act_name), collapse =  ";"), perl = TRUE),
                                                                                                                                                    prim_act_other = gsub("^(\\;)*|(?<=(\\;))(\\;)|(\\;)*$","",paste(unique(prim_act_other), collapse =  ";"), perl = TRUE),
                                                                                                                                                    yesno_snd = ifelse(any(yesno_snd %in% "Yes"), "Yes", "No"),
                                                                                                                                                    snd_act_name = gsub("^(\\;)*|(?<=(\\;))(\\;)|(\\;)*$","",paste(unique(snd_act_name), collapse =  ";"), perl = TRUE),
                                                                                                                                                    other_act_snd = gsub("^(\\;)*|(?<=(\\;))(\\;)|(\\;)*$","",paste(unique(other_act_snd), collapse =  ";"), perl = TRUE),
                                                                                                                                                    yesno_spec = ifelse(any(yesno_spec %in% "Yes"), "Yes", "No"),
                                                                                                                                                    spec_act_name = gsub("^(\\;)*|(?<=(\\;))(\\;)|(\\;)*$","",paste(unique(spec_act_name), collapse =  ";"), perl = TRUE),
                                                                                                                                                    other_act_spec = gsub("^(\\;)*|(?<=(\\;))(\\;)|(\\;)*$","",paste(unique(other_act_spec), collapse =  ";"), perl = TRUE),
                                                                                                                                                    comments = gsub("^(\\;)*|(?<=(\\;))(\\;)|(\\;)*$","",paste(unique(comments), collapse =  ";"), perl = TRUE),
                                                                                                                                                    park_scan_data_collection_complete = last(park_scan_data_collection_complete),
                                                                                                                                                    timestampPOSIX = first(timestampPOSIX),
                                                                                                                                                    datePOSIX = first(datePOSIX),
                                                                                                                                                    day = first(day),
                                                                                                                                                    month = first(month),
                                                                                                                                                    weekend = first(weekend)), ]
    if(nrow(TempCollapseResult) > 1 ) {
      stop(paste("Failed to compress observations, check result of", park, target_area, date, "between", starttime, "and", endtime))
    } else if(!exists("SOPARCCollapsedSubAreas")) {
      SOPARCCollapsedSubAreas <- TempCollapseResult
    } else {
      SOPARCCollapsedSubAreas <-  rbind(SOPARCCollapsedSubAreas, TempCollapseResult)
    }
    
    } else {
      stop("there are too many sub areas in scope. Need to resolve for proper assignment")
    }
  }
}


#combine observations without sub areas, and our newly collapsed sub areas, into a single DT
SOPARCObservations <- rbind(SOPARCObservations[sub_area == "",], SOPARCCollapsedSubAreas)




```

```{r save pre QA observations}
write.csv(SOPARCObservations,here("2023/outputs/data-metadata/PrepForQASOPARCObservations.csv"), row.names = FALSE)

```

## Quality Checks

The QA process attempts to identify and correct errors. The process initially performs a series of checks on all park data and reports results. For each park that fails QA, various strategies are executed to attempt to cure that park's data. The final results, including which strategies were executed, and the final QA status are saved in a csv file for review.

```{r generate initial QC report}

referenceDT <- SOPARCObservations[, .(areaCount = max(as.numeric(tar_area))), by = park_name]
observationsPerPark <- SOPARCObservations[, .(observationCount = .N), by = park_name]
parkStats <- merge(referenceDT, observationsPerPark)
parkStats[, expectedObservationCount := areaCount*24]
parkStats[, meetsExpected := ifelse(expectedObservationCount == observationCount, TRUE, FALSE)]

#insert base variables
parkStats[,`:=`(numberDaysObserved = NA_integer_,
                numberDaysOver =  NA_integer_,
                numberDaysUnder = NA_integer_,
                numberDaysComplete = NA_integer_,
                strateg1exec = FALSE),]


for(park in parkStats[,park_name]) {
  areasExpected <- parkStats[park_name == park, areaCount]
  observationsExpectedPerDay <- parkStats[park_name == park, expectedObservationCount] / 3
  
  #picking an arbitrary range for number of observations to be beyond flag
  x <- OBSINDAYTOLERANCE
  stratResult <- SOPARCObservations[park_name == park,
                                    .("N" = .N,
                                      "exact" = .N == observationsExpectedPerDay ,
                                      "inRange" = (.N >= observationsExpectedPerDay - x) & (.N <= observationsExpectedPerDay +x), 
                                      "under" = (.N < observationsExpectedPerDay), 
                                      "over" = (.N > observationsExpectedPerDay)),
                                    by = datePOSIX]
  parkStats[park_name == park,]$numberDaysObserved <- nrow(stratResult)
  parkStats[park_name == park,]$numberDaysComplete <- sum(stratResult$exact)
  parkStats[park_name == park,]$numberDaysOver <- sum(stratResult$over)
  parkStats[park_name == park,]$numberDaysUnder <-sum(stratResult$under)
}

parkStats[, passQA := ifelse(meetsExpected & 
                               numberDaysObserved == 3 &
                               numberDaysComplete == 3 &
                               numberDaysOver == 0 &
                               numberDaysUnder == 0, TRUE, FALSE)]

#insert base variables
# parkStats[,`:=`(strateg1exec = FALSE, 
#              numberDaysOver =  ifelse(meetsExpected, 0, NA),
#              numberDaysUnder = ifelse(meetsExpected, 0, NA),
#              numberDaysComplete = ifelse(meetsExpected, 3, NA)),]



# parkStats$strateg1exec <- FALSE
# parkStats$numberDaysOver <- 0
# parkStats$numberDaysUnder <- 0
# parkStats$numberDaysObserved <- 3
# parkStats$numberDaysComplete <- 3

#save QA results to file for review
write.csv(parkStats, here("2023/outputs/data-metadata/QA_report.csv"), row.names = F)


SOPARCObservationsOK <- SOPARCObservations[park_name %in% parkStats[passQA %in% TRUE, park_name],]
SOPARCObservationsToFix <- SOPARCObservations[park_name %in% parkStats[passQA %in% FALSE, park_name],]
write.csv(SOPARCObservationsOK, here("2023/outputs/data-metadata/QAPassSOPARCObservations.csv"), row.names = F)
write.csv(SOPARCObservationsToFix, here("2023/outputs/data-metadata/QAFailSOPARCObservations.csv"), row.names = F)



```

### Strategy 1: correct missingness where observations are only missing but correct number of days

This strategy looks at the number of days of data observed, and, if only 3 days are observed, then checks to confirm that, for each target area, 8 or fewer observations are made. If both of these conditions are met, this strategy attempts to insert the missing observations as blank entries in the part of the day that appeaer to be missing them. It identifies part of day by looking at each data collection period and inserting observations into the underweighted period(s) until 8 exists for each target area.

```{r any days with missing and only missing}




#for each day of observation with too many observations per day, try to resolve to 24xtarget area


for(park in unique(SOPARCObservationsToFix$park_name)) {
  SOPARCSingle <- SOPARCObservationsToFix[park_name == park,]
  areasExpected <- parkStats[park_name == park, areaCount]
  observationsExpectedPerDay <- parkStats[park_name == park, expectedObservationCount] / 3
  days <- parkStats[park_name == park, numberDaysObserved]
  #if only 3 days have 24*numberofarea +/- x observations, drop the other days and execute strategy 1 on remaining 3 days
  x <- OBSINDAYTOLERANCE
  stratResult <- SOPARCSingle[, .("N" = .N,"exact" = .N == observationsExpectedPerDay ,"inRange" = (.N >= observationsExpectedPerDay - x) & (.N <= observationsExpectedPerDay +x), "under" = (.N < observationsExpectedPerDay), "over" = (.N > observationsExpectedPerDay)),by = datePOSIX]

  if(parkStats[park_name == park,]$numberDaysObserved == 3) { #start strategy 1 checks: what to do if there are the correct number of days
    for(dateToFix in as.list(stratResult[exact == FALSE,]$datePOSIX)) { # per day
      if(all(SOPARCSingle[ datePOSIX == dateToFix, .N <= 8 , by = tar_area]$V1)) { #if no target area has more than the expected, insert missing observations into the day
        parkStats[park_name == park, ]$strateg1exec <- TRUE
        SOPARCSingleToFix <- SOPARCSingle[ datePOSIX == dateToFix, ]
        periodTracker <- 1
        #expectedTargetArea <- 1
        rowCounter <- 1
        nextExpectedTargetArea <- ifelse(areasExpected == 1, 1, 2)
        periodStartTimes <- c(0730, #start morning1
                         0800, #start morning2
                         1230, #start L1
                         1300, #start L2
                         1530, #start A1
                         1600, #start A2
                         1830, #start E1
                         1900) #start E2
        for(expectedTargetArea in 1:areasExpected) {
          if(nrow(SOPARCSingleToFix[tar_area == expectedTargetArea,]) != 8){
            numberToInsert <- 8 - nrow(SOPARCSingleToFix[tar_area == expectedTargetArea,])
            createEmptyObservation <- function(park, period, expectedTargetArea, areasExpected) {
                                    periodStartTimes <- c(0730, #start morning1
                         0800, #start morning2
                         1230, #start L1
                         1300, #start L2
                         1530, #start A1
                         1600, #start A2
                         1830, #start E1
                         1900) #start E2
                         timeToInterpolate <- round(periodStartTimes[periodTracker] + (30 * (expectedTargetArea/areasExpected))-(30 * (1/areasExpected)),0) #calculate how far into the observation period thetime should be set
            timeToInterpolateChar <- ifelse(nchar(as.character(timeToInterpolate)) == 3, paste0("0",substr(as.character(timeToInterpolate),1,1),":",substr(as.character(timeToInterpolate),2,3)), paste0(substr(as.character(timeToInterpolate),1,2),":",substr(as.character(timeToInterpolate),3,4)))
            timestampToInterpolate <- paste(as.character(dateToFix),timeToInterpolateChar)
                  tempObs <- data.table(record_id = "X",
                                     redcap_survey_identifier = "",
                                     park_scan_data_collection_timestamp = "",
                                     obs_initial = "APDE",
                                     start_time = timestampToInterpolate,
                                     park_name = park,
                                     tar_area = expectedTargetArea,
                                     sub_area = "",
                                     accessible = "",
                                     usable = "",
                                     lit = "",
                                     occupied = "",
                                     supervised = "",
                                     organized = "",
                                     equipped = "",
                                     num_child_prim = NA,
                                     num_child_snd = NA,
                                     num_child_spec = NA,
                                     num_teen_prim = NA,
                                     num_teen_snd = NA,
                                     num_teen_spec = NA,
                                     num_adult_prim = NA,
                                     num_adult_snd = NA,
                                     num_adult_spec = NA,
                                     num_senior_prim = NA,
                                     num_senior_snd = NA,
                                     num_senior_spec = NA,
                                     prim_act_name = "",
                                     prim_act_other = "",
                                     yesno_snd = "",
                                     snd_act_name = "",
                                     other_act_snd = "",
                                     yesno_spec = "",
                                     spec_act_name = "",
                                     other_act_spec = "",
                                     comments = "",
                                     park_scan_data_collection_complete = "",
                                     timestampPOSIX = as.POSIXct(strptime(timestampToInterpolate, "%Y-%m-%d %H:%M"), "America/Los_Angeles"),
                                     datePOSIX = as.Date(as.POSIXct(strptime(timestampToInterpolate, "%Y-%m-%d %H:%M"), "America/Los_Angeles"), tz = "America/Los_Angeles"),
                                     day = weekdays(as.POSIXct(strptime(timestampToInterpolate, "%Y-%m-%d %H:%M"), "America/Los_Angeles")),
                                     month = months(as.POSIXct(strptime(timestampToInterpolate, "%Y-%m-%d %H:%M"), "America/Los_Angeles")),
                                     weekend = ifelse(weekdays(as.POSIXct(strptime(timestampToInterpolate, "%Y-%m-%d %H:%M"), "America/Los_Angeles")) %in% c("Saturday", "Sunday"), 1, 0))
                  return(tempObs)
            }
            while(numberToInsert != 0){ # attempt to insert into the most underweighted time of day note, we don't need to worry about 1st or second period of that time (l1 vs l2)
              if(nrow(SOPARCSingleToFix[tar_area == expectedTargetArea & hour(timestampPOSIX) < 15,]) <= nrow(SOPARCSingleToFix[tar_area == expectedTargetArea & hour(timestampPOSIX) >= 15,])) { #first or second half of day
                if(nrow(SOPARCSingleToFix[tar_area == expectedTargetArea & hour(timestampPOSIX) < 15,][hour(timestampPOSIX) < 12,]) <= nrow(SOPARCSingleToFix[tar_area == expectedTargetArea & hour(timestampPOSIX) < 15,][hour(timestampPOSIX) >= 12,])) { #first or second of first half
                  #insert intro morning
                  periodTracker <- 1
                  SOPARCObservationsToFix <-  rbind(createEmptyObservation(park, periodTracker, expectedTargetArea, areasExpected),SOPARCObservationsToFix)
                  numberToInsert <- numberToInsert -1
                } else {
                  #insert into lunch
                  periodTracker <- 3
                  SOPARCObservationsToFix <-  rbind(createEmptyObservation(park, periodTracker, expectedTargetArea, areasExpected),SOPARCObservationsToFix)
  
                  numberToInsert <- numberToInsert -1
                }
              } else {
                if(nrow(SOPARCSingleToFix[tar_area == expectedTargetArea & hour(timestampPOSIX) >= 15,][hour(timestampPOSIX) < 18,]) <= nrow(SOPARCSingleToFix[tar_area == expectedTargetArea & hour(timestampPOSIX) >= 15,][hour(timestampPOSIX) >= 18,])) { #first or second of second half)
                  #insert into afternoon
                  periodTracker <- 5
                  SOPARCObservationsToFix <-  rbind(createEmptyObservation(park, periodTracker, expectedTargetArea, areasExpected),SOPARCObservationsToFix)
  
                  numberToInsert <- numberToInsert -1
                }else {
                  #insert into evening
                  periodTracker <- 7
                  SOPARCObservationsToFix <-  rbind(createEmptyObservation(park, periodTracker, expectedTargetArea, areasExpected),SOPARCObservationsToFix)
  
                  numberToInsert <- numberToInsert -1
                }
              }
              
  
            }
            
          }
        }
      }
    }
    
  } else {


  }
}

#update parkstats
referenceDTcheck <- SOPARCObservationsToFix[, .(areaCount = max(as.numeric(tar_area))), by = park_name]
observationsPerParkcheck <- SOPARCObservationsToFix[, .(observationCount = .N), by = park_name]
parkStatscheck <- merge(referenceDTcheck, observationsPerParkcheck)
parkStatscheck[, expectedObservationCount := areaCount*24]
parkStatscheck[, meetsExpected := ifelse(expectedObservationCount == observationCount, TRUE, FALSE)]

for(park in parkStatscheck$park_name) {
  parkStats[park_name == park, ]$areaCount <- parkStatscheck[park_name == park, areaCount]
  parkStats[park_name == park, ]$observationCount <- parkStatscheck[park_name == park, observationCount]
  parkStats[park_name == park, ]$expectedObservationCount <- parkStatscheck[park_name == park, expectedObservationCount]
  parkStats[park_name == park, ]$meetsExpected <- parkStatscheck[park_name == park, meetsExpected]
  #update metrics from above strategies
  #strategy 1 metrics
  x <- OBSINDAYTOLERANCE
  observationsExpectedPerDay <- parkStatscheck[park_name == park, expectedObservationCount] / 3
  stratResult <- SOPARCObservationsToFix[park_name == park, .("N" = .N,"exact" = .N == observationsExpectedPerDay ,"inRange" = (.N >= observationsExpectedPerDay - x) & (.N <= observationsExpectedPerDay +x), "under" = (.N < observationsExpectedPerDay), "over" = (.N > observationsExpectedPerDay)),by = datePOSIX]
  parkStats[park_name == park,]$numberDaysObserved <- nrow(stratResult)
  parkStats[park_name == park,]$numberDaysComplete <- sum(stratResult$exact)
  parkStats[park_name == park,]$numberDaysOver <- sum(stratResult$over)
  parkStats[park_name == park,]$numberDaysUnder <-sum(stratResult$under)
  
  
  parkStats[park_name == park, passQA := ifelse(meetsExpected &
                                                  numberDaysObserved == 3 &
                                                  numberDaysComplete == 3 &
                                                  numberDaysOver == 0 &
                                                  numberDaysUnder == 0, TRUE, FALSE)]
}

#write updated QA results
write.csv(parkStats, here("2023/outputs/data-metadata/QA_report.csv"), row.names = F)

SOPARCObservationsCorrected <- copy(SOPARCObservationsToFix[park_name %in% parkStats[passQA == TRUE, park_name],])

SOPARCObservationsOK <- rbind(SOPARCObservationsOK, SOPARCObservationsCorrected)
SOPARCObservationsToFix <- copy(SOPARCObservationsToFix[park_name %in% parkStats[passQA == FALSE, park_name],])
write.csv(SOPARCObservationsOK, here("2023/outputs/data-metadata/QAPassSOPARCObservations.csv"), row.names = F)
write.csv(SOPARCObservationsToFix, here("2023/outputs/data-metadata/QAFailSOPARCObservations.csv"), row.names = F)


```

When executing this script, the user may choose to use all data, or only data that have passed QA. It is generally suggested to only use data that have passed QA.

```{r select analysis set to use}
if(USEGOODDATA) {
  SOPARCAnalysisSet <- copy(SOPARCObservationsOK) #only use parks that have expected number of observation
} else {
  SOPARCAnalysisSet <- copy(rbind(SOPARCObservationsOK, SOPARCObservationsToFix)) #use all parks
}
```


## Parks Metadata

Metadata are provided for each park by Seth. The formal name, address, city, zip, neighborhood, tract, equity score, image status, planned park change notes, and general notes for each park will be appended to the analysis table.

```{r load parkwide metadata}
ParksMetaData <- as.data.table(read_xlsx(here("2023/inputs/Parks_MASTER.xlsx"), sheet = "Sheet1"))





```

### structure

`r summary(ParksMetaData)`

## Study Tracker

For the 2023 analysis we advised created a study tracker. For each time a park is studied, a new entry is created. Each entry should includ the park name, the date the study staretd, and a brief description of the study. We generate and save a study tracker that covers 2022 and 2023 studies.

In a later step we will use this table to append a study count to our parks

```{r create park study tracker}

#generate a list of full park names for 2023
parks2023 <- merge(parkStats, ParksMetaData, by.x = "park_name", by.y = "REDCap Name", all.x = T)$"Park Name"

#generate a list of full park names for 2022
lastYearParks <- c("Brighton Playfield",
                     "Cascade View Community Park",
                     "Marra Desimone Park",
                     "Midway Park",
                     "Moshier Memorial Park",
                     "Puget Sound Park",
                     "Roxhill Park",
                     "Steel Lake Park",
                     "Steve Cox Playfield",
                     "Tukwila Community Center")

ParksStudyRecord <- data.table("parkName" = lastYearParks, "studyStartDate" = "2022-06-25", "studyDescription" = "2022 annual study round 1")
ParksStudyRecord <- rbind(ParksStudyRecord, data.table("parkName" = lastYearParks, "studyStartDate" = "2022-07-21", "studyDescription" = "2022 annual study round 2"))
ParksStudyRecord <- rbind(ParksStudyRecord, data.table("parkName" = parks2023, "studyStartDate" = "2023-07-01", "studyDescription" = "2023 annual study"))

write.csv(ParksStudyRecord, here("2023/outputs/data-metadata/RecordOfParkStudies.csv"), row.names = F)
```

# Parameterizing Data For Analysis

## Time of day

For each observation we need to assign a time period to that observation. Because time stamps and the actual time of data collection may not align, the assignment of the time period requires some assumptions and may effect results.

```{r period assignment based on timestamp}
#| eval: false

expectedTimeBreakpoints <- c(0800, #start morning2
                             0830, #between M2 and L1
                             1230, #start L1
                             1300, #start L2
                             1330, # between L2 and A1
                             1530, #start A1
                             1600, #start A2
                             1630, #between A2 and E1
                             1830, #start E1
                             1900) #start E2


breakPointLabels <- c("morning1",
             "morning2",
             "betweenML",
             "lunch1",
             "lunch2",
             "betweenLA",
             "afternoon1",
             "afternoon2",
             "betweeenAE",
             "evening1",
             "evening2")



#for remaining observations, create a list of their times, standardized to POSIX
dataCollectionTimes <- as.POSIXct(strptime(SOPARCAnalysisSet$start_time, "%Y-%m-%d %H:%M"), "America/Los_Angeles")

breakpointAssignments <- breakPointLabels[findInterval(format(dataCollectionTimes, format = "%H%M"), expectedTimeBreakpoints, left.open = T, all.inside = FALSE, rightmost.closed = FALSE)+1]

#assign observed times to scheduled periods. Use only the house (otherwise would attribute date.)
SOPARCAnalysisSet$periodBasedOnBreakpoint <- breakpointAssignments

```

### Assigning time periods based on sequence

Our primary approach to assigning time periods relies on the assumption that timestamps are in order for each target area (e.g. the first observation of park A area 1 is always "morning1", the second, "morning2" and so on.)

In turn, with this approach, we assign the sequence of the day such that the first morning1 is day 1, the second is day2, and so on.

We assume that there are the correct number of observation periods such that 1 can be assigned to each period, and 8 to each day.

For each park, there should be 3 days of observation. We will detect this by extracting the date per park and putting observations in order of day "1" "2" "3".

```{r assign time periods based on sequence}

sequenceLabels <- c("morning1",
             "morning2",
             "lunch1",
             "lunch2",
             "afternoon1",
             "afternoon2",
             "evening1",
             "evening2")


#apply observation period labels based on the order that a park's target area was observed
SOPARCAnalysisSet[order(timestampPOSIX, tar_area), periodBasedOnSequence := rep_len(sequenceLabels, .N), by = .(park_name)]


#apply day sequence labels based on the order of a park's observation period
SOPARCAnalysisSet[order(datePOSIX), dayNumberBasedOnSequence := as.numeric(factor(datePOSIX)), by = .(park_name)]

```

Finally, we cross check each day and period assignment to confirm that: \* all all parks have the expected number of period observations (e.g. if a park has 4 target areas, it should have 12 morning1 observations). \* all observations assigned to a particular day-count (e.g. "1") actually are on the same date (this assumption may need to be relaxed or data corrected before all data pass QA). \* that each day has the expected number of observations.

```{r QA day assignments}

PreQADT <- copy(SOPARCAnalysisSet)

#check if all expected time periods exists and are assigned
for(park in unique(PreQADT$park_name)) {
  multiple <- referenceDT[park_name %in% park,]$areaCount
  #check sequence based times
  for(period in sequenceLabels) {
    if(nrow(PreQADT[park_name %in% park & periodBasedOnSequence %in% period, ]) != multiple*3) { 
      PreQADT[park_name %in% park & periodBasedOnSequence %in% period, PeriodBasedOnSequencePass := 0]
    } else {
      PreQADT[park_name %in% park & periodBasedOnSequence %in% period, PeriodBasedOnSequencePass := 1]
    }
    
  }
  for(daycount in 1:3){
    #we will perform 2 checks here
    #1 review if the correct number of observations are assigned to each day
    if(nrow(PreQADT[park_name %in% park & dayNumberBasedOnSequence %in% daycount, ]) != multiple*8) { 
      PreQADT[park_name %in% park & dayNumberBasedOnSequence %in% daycount, DayCountBasedOnSequencePass := 0]
    } else {
      PreQADT[park_name %in% park & dayNumberBasedOnSequence %in% daycount, DayCountBasedOnSequencePass := 1]
    }
    #2 confirm that the observations assigned to the same day are flagged with the same day of the week
    if(length(unique(PreQADT[park_name %in% park & dayNumberBasedOnSequence %in% daycount, day])) != 1) { 
      PreQADT[park_name %in% park & dayNumberBasedOnSequence %in% daycount, DayAssignmentBasedOnSequencePass := 0]
    } else {
      PreQADT[park_name %in% park & dayNumberBasedOnSequence %in% daycount, DayAssignmentBasedOnSequencePass := 1]
    }
  }
  
}

PostQADT <- copy(PreQADT)

#check line items results. update QAPass status
QAVarList <- quote(list(PeriodBasedOnSequencePass,
                        DayCountBasedOnSequencePass,
                        DayAssignmentBasedOnSequencePass))

#check that observation periods have been properly applied
passPeriodQA <- unique(PostQADT[apply(PostQADT[,eval(QAVarList)], 1, mean) == 1, park_name])

#update QA tracker

parkStats[,`:=`(periodsApplied = ifelse(park_name %in% passPeriodQA,TRUE,FALSE),
             analysisReady = ifelse(park_name %in% passPeriodQA & passQA,TRUE,FALSE))]

write.csv(ParksStudyRecord, here("2023/outputs/data-metadata/QA_report.csv"), row.names = F)

```

## aggregation of periods for analysis

For all analyses, we do not want to distinguish between the first and second of a period (e.g. morning1 and morning2). We aggregate these according to the following:

| positive indicaters | aggregate to |
|---------------------|--------------|
| accessible          | Yes          |
| usable              | Yes          |
| lit                 | Yes          |
| occupied            | Yes          |
| supervised          | Yes          |
| organized           | Yes          |
| equipped            | Yes          |

| counts of       | are aggregated as |
|-----------------|-------------------|
| num_child_prim  | ceiling of mean   |
| num_child_snd   | ceiling of mean   |
| num_child_spec  | ceiling of mean   |
| num_teen_prim   | ceiling of mean   |
| num_teen_snd    | ceiling of mean   |
| num_teen_spec   | ceiling of mean   |
| num_adult_prim  | ceiling of mean   |
| num_adult_snd   | ceiling of mean   |
| num_adult_spec  | ceiling of mean   |
| num_senior_prim | ceiling of mean   |
| num_senior_snd  | ceiling of mean   |
| num_senior_spec | ceiling of mean   |

| open text field of | aggregate to     |
|--------------------|------------------|
| prim_act_name      | concantinate ";" |
| prim_act_other     | concantinate ";" |
| snd_act_name       | concantinate ";" |
| other_act_snd      | concantinate ";" |
| spec_act_name      | concantinate ";" |
| other_act_spec     | concantinate ";" |
| comments           | concantinate ";" |

```{r period aggregation}

if(USEGOODDATA) {
  SOPARCtoAggregate <- copy(SOPARCAnalysisSet[park_name %in% parkStats[analysisReady == TRUE, park_name],])
} else {
  SOPARCtoAggregate <- copy(SOPARCAnalysisSet)
}

#create single periodname for aggregating first and second of each period using sequence periods
SOPARCtoAggregate[periodBasedOnSequence %in% c("morning1","morning2"), period := "Morning"]
SOPARCtoAggregate[periodBasedOnSequence %in% c("lunch1","lunch2"), period := "Lunch"]
SOPARCtoAggregate[periodBasedOnSequence %in% c("afternoon1","afternoon2"), period := "Afternoon"]
SOPARCtoAggregate[periodBasedOnSequence %in% c("evening1","evening2"), period := "Evening"]

SOPARCtoAggregate$period <-  factor(SOPARCtoAggregate$period, level = c("Morning","Lunch","Afternoon","Evening"))


SOPARCAggregated <- SOPARCtoAggregate[,
                                      .("accessible" = ifelse(any(accessible %in% "Yes"), "Yes", "No"),
                                        "usable" = ifelse(any(usable %in% "Yes"), "Yes", "No"),
                                        "lit" = ifelse(any(lit %in% "Yes"), "Yes", "No"),
                                        "occupied" = ifelse(any(occupied %in% "Yes"), "Yes", "No"),
                                        "supervised" = ifelse(any(supervised %in% "Yes"), "Yes", "No"),
                                        "organized" = ifelse(any(organized %in% "Yes"), "Yes", "No"),
                                        "equipped" = ifelse(any(organized %in% "Yes"), "Yes", "No"),
                                        "num_child_prim" = ceiling(mean(num_child_prim, na.rm = T, )),
                                        "num_child_snd" = ceiling(mean(num_child_snd, na.rm = T)),
                                        "num_child_spec" = ceiling(mean(num_child_spec, na.rm = T)),
                                        "num_teen_prim"= ceiling(mean(num_teen_prim, na.rm = T)),
                                        "num_teen_snd" = ceiling(mean(num_teen_snd, na.rm = T)),
                                        "num_teen_spec" = ceiling(mean(num_teen_spec, na.rm = T)),
                                        "num_adult_prim" = ceiling(mean(num_adult_prim, na.rm = T)),
                                        "num_adult_snd" = ceiling(mean(num_adult_snd, na.rm = T)),
                                        "num_adult_spec" = ceiling(mean(num_adult_spec, na.rm = T)),
                                        "num_senior_prim" = ceiling(mean(num_senior_prim, na.rm = T)),
                                        "num_senior_snd" = ceiling(mean(num_senior_snd, na.rm = T)),
                                        "num_senior_spec" = ceiling(mean(num_senior_spec, na.rm = T))), 
                                      by = .(park_name, tar_area, dayNumberBasedOnSequence, period, datePOSIX, day, month, weekend )]

#add columns for each activity



```

```{r add metadata and save analysis sets}


# add provided metadata to aggregated and non aggregated analysis ready sets
SOPARCtoAggregate <- merge(SOPARCtoAggregate, ParksMetaData[,.(`REDCap Name`,
                                                  "park_name_full" = `Park Name`, 
                                                  `Park Address`,
                                                  City,
                                                  Zip,
                                                  Neighborhood,
                                                  Images,
                                                  Tract,
                                                  `Equity Score Priority `,
                                                  `Planned park changes?`,
                                                  Notes)], by.x = "park_name", by.y = "REDCap Name", all.x = T)

SOPARCAggregated <- merge(SOPARCAggregated, ParksMetaData[,.(`REDCap Name`,
                                                  "park_name_full" = `Park Name`, 
                                                  `Park Address`,
                                                  City,
                                                  Zip,
                                                  Neighborhood,
                                                  Images,
                                                  Tract,
                                                  `Equity Score Priority `,
                                                  `Planned park changes?`,
                                                  Notes)], by.x = "park_name", by.y = "REDCap Name", all.x = T)

#generate study count and append this to aggregated and non aggregated analysis ready sets
SOPARCtoAggregate <- merge(SOPARCtoAggregate, ParksStudyRecord[,.("study_count" = .N) , by = parkName], by.x = "park_name_full", by.y = "parkName", all.x = T)
SOPARCAggregated <- merge(SOPARCAggregated, ParksStudyRecord[,.("study_count" = .N) , by = parkName], by.x = "park_name_full", by.y = "parkName", all.x = T)

#save results
write.csv(SOPARCtoAggregate, here("2023/outputs/data-metadata/SOPARCAnalysisSet.csv"), row.names = FALSE)
write.csv(SOPARCAggregated, here("2023/outputs/data-metadata/SOPARCAnalysisSetAggregatedPeriods.csv"), row.names = FALSE)


#would like to save as xlsx, but NAn ot handled gracefully
#write_xlsx(SOPARCtoAggregate, here("2023/outputs/data-metadata/SOPARCAnalysisSet.xlsx"), row.names = FALSE, showNA = FALSE)
#write_xlsx(SOPARCAggregated, here("2023/outputs/data-metadata/SOPARCAnalysisSetAggregatedPeriods.xlsx"), row.names = FALSE)

```

# Results

Results are saved in two locations, an excel workbook that contains a separate page for each table of analysis results, and this document which contains a human readable report format. Note that the charts in the excel book are much large and more appropriate for reproduction

```{r create excel workbook data structure for export}
WB <- openxlsx2::wb_workbook()


```

## Included Parks

```{r report included parks}
#| output: true

kbl(SOPARCAggregated[order(park_name), .("Park Name" = unique(park_name))], 
    caption = "Parks Included In Analysis",
    booktabs = TRUE,
    longtable = TRUE) %>% kable_styling(latex_options = c("striped", "repeat_header", "hold_position"))

```

## Attendance

### Number of Attendees

Not reported. See note.

Notes:

-   number of attendees is likely to contain duplicate counts because of the way to people were counted within an observation. For example, if 2 people are in a target area at observation T_1, and then 3 in T_2, there is no reliable way to identify if these were 5 different people, or 3 different people.

```{r number of attendees chart}
#| output: false

# kbl(SOPARCtoAggregate[order(park_name),.("Number of Attendees" = sum(num_child_prim, num_child_snd, num_child_spec, num_teen_prim, num_teen_snd, num_teen_spec, num_adult_prim, num_adult_snd, num_adult_spec, num_senior_prim, num_senior_snd, num_senior_spec, na.rm = TRUE)), by = .("Park Name" = park_name)], 
#       caption =  "Total Number of Attendees",
#     booktabs = T) %>% kable_styling(latex_options = "striped")
DT <- SOPARCAggregated[order(park_name),.("Number of Attendees" = sum(num_child_prim, num_child_snd, num_child_spec, num_teen_prim, num_teen_snd, num_teen_spec, num_adult_prim, num_adult_snd, num_adult_spec, num_senior_prim, num_senior_snd, num_senior_spec, na.rm = TRUE)), by = .("Park Name" = park_name)]

kbl(DT, 
      caption =  "Total Number of Attendees",
    booktabs = TRUE,
    longtable = TRUE) %>% kable_styling(latex_options = c("striped", "repeat_header", "hold_position"))



```

### Average Park Visitors by Time

Notes:

```{r average park visitors by time, fig.cap = "Average Park Visitors by Time"}
#| output: true


# kbl(SOPARCtoAggregate[order(park_name),.("Attendees" = round(sum(num_child_prim, num_child_snd, num_child_spec, num_teen_prim, num_teen_snd, num_teen_spec, num_adult_prim, num_adult_snd, num_adult_spec, num_senior_prim, num_senior_snd, num_senior_spec, na.rm = TRUE)/6 ,0)), by = .("Park Name" = park_name, "Time Period" = period)],
#     caption = "Daily Average Number of Attendees Per Time Period",
#     booktabs = TRUE,
#     longtable = TRUE) %>% kable_styling(latex_options = c("striped", "repeat_header", "hold_position"))
DT <- SOPARCAggregated[order(park_name),.("Attendees" = round(sum(num_child_prim, num_child_snd, num_child_spec, num_teen_prim, num_teen_snd, num_teen_spec, num_adult_prim, num_adult_snd, num_adult_spec, num_senior_prim, num_senior_snd, num_senior_spec, na.rm = TRUE)/3 ,0)), by = .("Park Name" = park_name, "Time Period" = period)]

WB$add_worksheet("Avg Visitors x Time Data")
WB$add_data(sheet = "Avg Visitors x Time Data", DT)



PT <- ggplot(DT, (aes(x = `Time Period`, y = Attendees))) +
  geom_col() +
  facet_wrap(vars(DT$"Park Name"), scales = "free")
   #theme(axis.text.x = element_text(angle = 0, vjust = 0, hjust=0))

# WB$add_worksheet("Avg Visitors x Time Plot")
print(PT)
# WB$add_plot(sheet = "Avg Visitors x Time Plot", width = 20, height = 20)

#save charts as individual files
#cleanup old images of this chart
if(file.exists(here("./2023/outputs/charts/avg_visitors_x_time/"))) {
  unlink(here("./2023/outputs/charts/avg_visitors_x_time/"), recursive = T)
}
dir.create(here("./2023/outputs/charts/avg_visitors_x_time"), recursive = T)

maxCategory <- (1)

for(park in unique(SOPARCAggregated$park_name)) {
  
  if(max(DT[`Park Name` %in% park]$Attendees) < 11) {
    topOfY <- 10
  } else if(max(DT[`Park Name` %in% park]$Attendees) < 51){
    topOfY <- 50
  } else if(max(DT[`Park Name` %in% park]$Attendees) < 101){
    topOfY <- 100
  } else if(max(DT[`Park Name` %in% park]$Attendees) < 501){
    topOfY <- 500
  } else if(max(DT[`Park Name` %in% park]$Attendees) < 1001) {
    topOfY <- 1000
  } else {
    topOfY <- max(DT[`Park Name` %in% park]$Attendees)
  }
  
  PTExport <- ggplot(DT[`Park Name` %in% park], (aes(x = `Time Period`, y = `Attendees`))) +
  geom_col()  +
  labs(title = paste("Daily Average Park Visitors"), subtitle = park) +
  scale_y_continuous(limits=c(0,topOfY)) +
  theme(plot.title = element_text(size = 14),
        plot.subtitle = element_text(size = 12,),
        axis.title.x = element_text(size = 12))
  
  ggsave(paste0(park,"_avg_visitors_x_time.pdf"), plot = PTExport, path = paste0("./outputs/charts/avg_visitors_x_time"),width = CHARTWIDTH, height = CHARTHEIGHT)
  ggsave(paste0(park,"_avg_visitors_x_time.png"), plot = PTExport, path = paste0("./outputs/charts/avg_visitors_x_time"),width = CHARTWIDTH, height = CHARTHEIGHT)
}


kbl(DT,
    caption = "Daily Average Number of Attendees Per Time Period",
    booktabs = TRUE,
    longtable = TRUE) %>% kable_styling(latex_options = c("striped", "repeat_header", "hold_position"))
```

### Rate of Park Use By Time

Notes:

-   Rate is within-park (each park totals to 100%).

```{r Rate of Park Use By Time, fig.cap="Rate Of Park Use By Time"}
#| output: true


DT <- SOPARCAggregated[order(park_name), sum(num_child_prim, num_child_snd, num_child_spec, num_teen_prim, num_teen_snd, num_teen_spec, num_adult_prim, num_adult_snd, num_adult_spec, num_senior_prim, num_senior_snd, num_senior_spec, na.rm = TRUE)/3 , by = .(park_name, period)][,.("Time Period" = period, "Proportion of Use" = round((V1/ sum(V1)),2)), by = .("Park Name" = park_name) ]

WB$add_worksheet("Rate Use x Time Data")
WB$add_data(sheet = "Rate Use x Time Data", DT)

PT <- ggplot(DT, (aes(x = `Time Period`, y = `Proportion of Use`))) +
  geom_col() +
  facet_wrap(vars(`Park Name`), scales = "free") +
  #scale_y_continuous(limits=c(0,1)) +
  scale_y_continuous(limits = c(0,1),labels = scales::percent) +
   theme(axis.text.x = element_text(angle = 0, vjust = 0, hjust=0))

# WB$add_worksheet("Avg Visitors x Time Plot")
print(PT)
# WB$add_plot(sheet = "Avg Visitors x Time Plot", width = 20, height = 20)


#save charts as individual files
#cleanup old images of this chart
if(file.exists(here("./2023/outputs/charts/rate_visitors_x_time/"))) {
  unlink(here("./2023/outputs/charts/rate_visitors_x_time/"), recursive = T)
}
dir.create(here("./2023/outputs/charts/rate_visitors_x_time"), recursive = T)

for(park in unique(SOPARCAggregated$park_name)) {
  PTExport <- ggplot(DT[`Park Name` %in% park], (aes(x = `Time Period`, y = `Proportion of Use`))) +
  geom_col()  +
  labs(title = paste("Daily Rate Of Park Use"), subtitle = park) +
  scale_y_continuous(limits=c(0,1), labels = scales::percent) +
  theme(plot.title = element_text(size = 14),
        plot.subtitle = element_text(size = 12,),
        axis.title.x = element_text(size = 12))
  
  ggsave(paste0(park,"_rate_visitors_x_time.pdf"), plot = PTExport, path = paste0("./outputs/charts/rate_visitors_x_time"),width = CHARTWIDTH, height = CHARTHEIGHT)
  ggsave(paste0(park,"_rate_visitors_x_time.png"), plot = PTExport, path = paste0("./outputs/charts/rate_visitors_x_time"),width = CHARTWIDTH, height = CHARTHEIGHT)
}

kbl(DT,
    caption = "Daily Rate Of Park Use By Time Period",
    booktabs = TRUE,
    longtable = TRUE) %>% kable_styling(latex_options = c("striped", "repeat_header", "hold_position"))



```

### Distribution of Attendee Ages

Notes:

```{r distribution of attendee ages, fig.cap="Distribution of Attendee Ages"}
#| output: true

DT <- SOPARCAggregated[order(park_name), .( "Age Group" = rep(c("child","teen","adult","senior")), 
                                           "Daily Average Number of People" = round(c(sum(num_child_prim, 
                                                                                    num_child_snd,
                                                                                    num_child_spec, 
                                                                                    na.rm = TRUE)/3,
                                                                                sum(num_teen_prim, 
                                                                                    num_teen_snd, 
                                                                                    num_teen_spec, 
                                                                                    na.rm = TRUE)/3,
                                                                                sum(num_adult_prim, 
                                                                                    num_adult_snd, 
                                                                                    num_adult_spec, 
                                                                                    na.rm = TRUE)/3,
                                                                                sum(num_senior_prim, 
                                                                                    num_senior_snd, 
                                                                                    num_senior_spec, 
                                                                                    na.rm = TRUE)/3))), by = .("Park Name" = park_name)]


WB$add_worksheet("Avg Attend x Age")
WB$add_data(sheet = "Avg Attend x Age", DT)

PT <- ggplot(DT, (aes(x = `Age Group`, y = `Daily Average Number of People`))) +
  geom_col() +
  facet_wrap(vars(`Park Name`), scales = "free") +
   theme(axis.text.x = element_text(angle = 0, vjust = 0, hjust=0))

# WB$add_worksheet("Avg Visitors x Time Plot")
print(PT)
# WB$add_plot(sheet = "Avg Visitors x Time Plot", width = 20, height = 20)

kbl(DT,
    caption = "Daily Average Number of Attendees by Age Group",
    booktabs = TRUE,
    longtable = TRUE) %>% kable_styling(latex_options = c("striped", "repeat_header", "hold_position"))

```

### Rate of Park Use By Age

Notes:

-   Rate is within-park (each park totals to 100%).

```{r rate of park use by age, fig.cap="Rate of Park Use By Age"}
#| output: true


DT <- SOPARCAggregated[order(park_name), .("Age Group" = rep(c("child","teen","adult","senior")),
                                         "Percent Use" = 
                                           round(c(
                                             sum(num_child_prim, num_child_snd, num_child_spec, na.rm = TRUE) / sum(num_child_prim, num_child_snd, num_child_spec, num_teen_prim, num_teen_snd, num_teen_spec, num_adult_prim, num_adult_snd, num_adult_spec, num_senior_prim, num_senior_snd, num_senior_spec, na.rm = TRUE),
                                             sum(num_teen_prim, num_teen_snd, num_teen_spec, na.rm = TRUE) / sum(num_child_prim, num_child_snd, num_child_spec, num_teen_prim, num_teen_snd, num_teen_spec, num_adult_prim, num_adult_snd, num_adult_spec, num_senior_prim, num_senior_snd, num_senior_spec, na.rm = TRUE),
                                             sum(num_adult_prim, num_adult_snd, num_adult_spec, na.rm = TRUE) / sum(num_child_prim, num_child_snd, num_child_spec, num_teen_prim, num_teen_snd, num_teen_spec, num_adult_prim, num_adult_snd, num_adult_spec, num_senior_prim, num_senior_snd, num_senior_spec, na.rm = TRUE),
                                             sum(num_senior_prim, num_senior_snd, num_senior_spec, na.rm = TRUE) / sum(num_child_prim, num_child_snd, num_child_spec, num_teen_prim, num_teen_snd, num_teen_spec, num_adult_prim, num_adult_snd, num_adult_spec, num_senior_prim, num_senior_snd, num_senior_spec, na.rm = TRUE))*100)), by = .("Park Name" = park_name)]


WB$add_worksheet("Rate Use x Age")
WB$add_data(sheet = "Rate Use x Age", DT)

PT <- ggplot(DT, (aes(x = factor(`Age Group`, levels = c("child","teen","adult","senior")), y = `Percent Use`))) +
  xlab("Age Group") +
  geom_col() +
  facet_wrap(vars(`Park Name`)) +
   theme(axis.text.x = element_text(angle = 0, vjust = 0, hjust=0))

# WB$add_worksheet("Avg Visitors x Time Plot")
print(PT)
# WB$add_plot(sheet = "Avg Visitors x Time Plot", width = 20, height = 20)

kbl(DT,
    caption = "Rate of Park Use by Age Group",
    booktabs = TRUE,
    longtable = TRUE) %>% kable_styling(latex_options = c("striped", "repeat_header", "hold_position"))

```

## Occupancy

NOTES:

-   percentage of the 24 observation periods where the park had any occupancy Doing this by target areas has problem because the content of target areas is not stable.
-   this metric needs to be QA'd

```{r analysis of occupancy rate}
#| output: true

DT <- SOPARCtoAggregate[order(park_name), ifelse(any(occupied %in% "Yes"), 1, 0), by = .(park_name, periodBasedOnSequence,dayNumberBasedOnSequence)][, .( "Percent of time Occupied" =round((sum(V1)/24 )*100,0)),by=.("Park Name" = park_name)]

WB$add_worksheet("Rate Occupied")
WB$add_data(sheet = "Rate Occupied", DT)


kbl(DT,
    caption = "Occupancy Rate",
    booktabs = TRUE,
    longtable = TRUE) %>% kable_styling(latex_options = c("striped", "repeat_header", "hold_position"))



```

## Utility

Notes: \* activities are currently not properly handled for sub areas. Need to improve parsing for this to pass QA \* counts of people doing activities are unreliable as there may be double counting of individuals

```{r Number of people doing an activity in a park}
#| output: false

#this metric doesn't make sense given double counting of target areas
DT <- rbind(SOPARCtoAggregate[,sum(num_child_prim, num_teen_prim, num_adult_prim, num_senior_prim, na.rm = T) , by = .(park_name,activity = prim_act_name)][V1 > 0,],
SOPARCtoAggregate[,sum(num_child_snd, num_teen_snd, num_adult_snd, num_senior_snd, na.rm = T) , by = .(park_name,activity = snd_act_name)][V1 > 0,],
SOPARCtoAggregate[,sum(num_child_spec, num_teen_spec, num_adult_spec, num_senior_spec, na.rm = T) , by = .(park_name,activity = spec_act_name)][V1 > 0,])[,.("Number of People" = sum(V1)), by = .("Park Name" = park_name,"Activity" = activity)]



kbl(DT[order(`Park Name`)],
    caption = "Rate of Park Use by Age Group",
    booktabs = TRUE,
    longtable = TRUE) %>% kable_styling(latex_options = c("striped", "repeat_header", "hold_position"))
```

```{r Frequency of Activities Per Park}
#| output: true

DT <- rbind(SOPARCtoAggregate[,sum(num_child_prim, num_teen_prim, num_adult_prim, num_senior_prim, na.rm = T) , by = .(park_name, dayNumberBasedOnSequence,periodBasedOnSequence,activity = prim_act_name)][V1 > 0,],
SOPARCtoAggregate[,sum(num_child_snd, num_teen_snd, num_adult_snd, num_senior_snd, na.rm = T) , by = .(park_name, dayNumberBasedOnSequence,periodBasedOnSequence,activity = prim_act_name)][V1 > 0,],
SOPARCtoAggregate[,sum(num_child_spec, num_teen_spec, num_adult_spec, num_senior_spec, na.rm = T) , by = .(park_name, dayNumberBasedOnSequence,periodBasedOnSequence,activity = prim_act_name)][V1 > 0,])[,sum(V1), by = .(park_name, dayNumberBasedOnSequence,periodBasedOnSequence,activity)][order(park_name) ,.("Rate" = paste0(round((.N / 24) *100),"%")), by = .("Park Name" = park_name,"Activity" = activity)]

WB$add_worksheet("Activity Rate")
WB$add_data(sheet = "Activity Rate", DT)

kbl(DT,
    caption = "Rate of Activities Observed Per Park",
    booktabs = TRUE,
    longtable = TRUE) %>% 
  kable_styling(latex_options = c("striped", "repeat_header", "hold_position"))
```

```{r save workbook}

WB$save("./outputs/tables/analysis_data.xlsx", overwrite = TRUE)
```

```{r}
#| eval: false
#| echo: false  

#resolve redundant observations
for(park in unique(SOPARCObservations[,park_name])) {
  for(area in 1:max(SOPARCObservations[park_name == park, tar_area])) {
    #check if the expected number of observations exists for each area
    if(all(SOPARCObservations[park_name == park & tar_area == area, sub_area ] == "")) { #testing areas where all observations are without sub areas
      if(!nrow(SOPARCObservations[park_name == park & tar_area == area,] ) == 24) { #checks that there are exactly 24 observations. Note, this skips any that have 24 to instead assert sequential observation order. That may be an awful assumption
        #check that there are 3 days and 8 observations per day for each target area of the park
        if(length(unique(SOPARCObservations[park_name == park & tar_area == area,]$datePOSIX)) == 3){           #checks for those that have 3 days...

          #confirm that each target area has 8 observations per day, and make corrections if not
          for(day in unique(SOPARCObservations[park_name == park & tar_area == area,]$idate)) {
            if(nrow(SOPARCObservations[park_name == park & tar_area == area & idate == day,]) > 8) {
              numberToFix <- nrow(SOPARCObservations[park_name == park & tar_area == area & idate == day,]) - 8
              #things to do to resolve when there are extra observations in a day
              print(paste("there are", numberToFix, "too many observations in", park, area, as.Date(day)))
              print("attempting to resolve by temporal proximity and data similarity")
              #where X is the number of excess observations, choose X_1 where X_1 is an entry that matches at least one other entry in all regards except timestamp and ID
              #if more than 2 identical items exists within a +/- 30m timespan of X_1, delete X_1
              #X-1, if X >0 repeat
              duplicates <- SOPARCObservations[park_name == park & tar_area == area & as.Date(timestampPOSIX, tz = "America/Los_Angeles") == day,record_id][duplicated(SOPARCObservations[park_name == park & tar_area == area & as.Date(timestampPOSIX, tz = "America/Los_Angeles") == day, !c("record_id","start_time","timestampPOSIX", "itimeOfDay")],by = )]
              duplicateseWithNearMatches <- NA
              
              #check sequentially if there are 3 items within 30 minutes of each other, and one is identical
              for( id in duplicates) {
                varsToMatchOn <- c("park_name",
                                   "tar_area",
                                   "sub_area",
                                   "accessible",
                                   "usable",
                                   "lit",
                                   "occupied",
                                   "supervised",
                                   "organized",
                                   "equipped",
                                   "num_child_prim",
                                   "num_child_snd",
                                   "num_child_spec",
                                   "num_teen_prim",
                                   "num_teen_snd",
                                   "num_teen_spec",
                                   "num_adult_prim",
                                   "num_adult_snd",
                                   "num_adult_spec",
                                   "num_senior_prim",
                                   "num_senior_snd",
                                   "num_senior_spec",
                                   "prim_act_name",
                                   "prim_act_other",
                                   "yesno_snd",
                                   "snd_act_name",
                                   "other_act_snd",
                                   "yesno_spec",
                                   "spec_act_name",
                                   "other_act_spec",
                                   "comments")
                accessible_check <- SOPARCObservations[record_id == id]$accessible
                usable_check <- SOPARCObservations[record_id == id]$usable
                lit_check <- SOPARCObservations[record_id == id]$lit
                occupied_check <- SOPARCObservations[record_id == id]$occupied
                supervised_check <- SOPARCObservations[record_id == id]$supervised
                organized_check <- SOPARCObservations[record_id == id]$organized
                equipped_check <- SOPARCObservations[record_id == id]$equipped
                num_child_prim_check <- SOPARCObservations[record_id == id]$num_child_prim
                num_child_snd_check <- SOPARCObservations[record_id == id]$num_child_snd
                num_child_spec_check <- SOPARCObservations[record_id == id]$num_child_spec
                num_teen_prim_check <- SOPARCObservations[record_id == id]$num_teen_prim
                num_teen_snd_check <- SOPARCObservations[record_id == id]$num_teen_snd
                num_teen_spec_check <- SOPARCObservations[record_id == id]$num_teen_spec
                num_adult_prim_check <- SOPARCObservations[record_id == id]$num_adult_prim
                num_adult_snd_check <- SOPARCObservations[record_id == id]$num_adult_snd
                num_adult_spec_check <- SOPARCObservations[record_id == id]$num_adult_spec
                num_senior_prim_check <- SOPARCObservations[record_id == id]$num_senior_prim
                num_senior_snd_check <- SOPARCObservations[record_id == id]$num_senior_snd
                num_senior_spec_check <- SOPARCObservations[record_id == id]$num_senior_spec
                prim_act_name_check <- SOPARCObservations[record_id == id]$prim_act_name
                prim_act_other_check <- SOPARCObservations[record_id == id]$prim_act_other
                yesno_snd_check <- SOPARCObservations[record_id == id]$yesno_snd
                snd_act_name_check <- SOPARCObservations[record_id == id]$snd_act_name
                other_act_snd_check <- SOPARCObservations[record_id == id]$other_act_snd
                yesno_spec_check <- SOPARCObservations[record_id == id]$yesno_spec
                spec_act_name_check <- SOPARCObservations[record_id == id]$spec_act_name
                other_act_spec_check <- SOPARCObservations[record_id == id]$other_act_spec
                comments_check <- SOPARCObservations[record_id == id]$comments
                
                temporalmid <- SOPARCObservations[record_id == id, itimeOfDay]
                print(paste("searching for records within +/- 30m of record", id))
                print(paste(SOPARCObservations[park_name == park & tar_area == area & idate == day & record_id != id & (itimeOfDay > temporalmid - 1800 & itimeOfDay < temporalmid + 1800),]$record_id, "was found"))
                #duplicated(SOPARCObservations[park_name == park & tar_area == area & idate == day,],by =   varsToMatchOn)

                #SOPARCObservations[park_name == park & tar_area == area & as.Date(timestampPOSIX, tz = "America/Los_Angeles") == day & (itimeOfDay > temporalmid - 1800 & itimeOfDay < temporalmid + 1800),]
                
              }

              SOPARCObservations[park_name == park & tar_area == area,]

              while(numberToFix > 0) {
                
              }
              
              
            }
            if(nrow(SOPARCObservations[park_name == park & tar_area == area & as.Date(timestampPOSIX, tz = "America/Los_Angeles") == day,]) < 8) {
              print(paste("there are too few observations in", park, area, as.Date(day, tz = "America/Los_Angeles")))
            }
          }
          
        } else if(length(unique(as.Date(SOPARCObservations[park_name == park & tar_area == area,]$timestampPOSIX, tz = "America/Los_Angeles"))) > 3){
          print(paste("more than 3 days detected for", park, area))
        } else if((length(unique(as.Date(SOPARCObservations[park_name == park & tar_area == area,]$timestampPOSIX, tz = "America/Los_Angeles"))) < 3)) {
          print(paste("less than 3 days detected for", park, area))
        } else {
          print(paste("undefined date error found in", park, area))
        }
        
      }
    } else { #testing observations where any of the area has sub areas
      #for(subarea in SOPARCObservations[park_name == park & tar_area == area, sub_area ]) {
    }
  }
}
```
