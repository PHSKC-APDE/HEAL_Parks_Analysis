---
title: "HEAL Parks 2023 Analysis"
author: "Ronald Buie"
date-modified: "`r Sys.Date()`"

format: 
  pdf:
    toc: true
    number-sections: true
---

# Front Matter

## ITHS REDCap

Data are currently stored in ITHS REDCap. This is currently managed by the program managers and Ronald. This script will pull from the project if correct information is provided (see **secrets and tokens** in the **Setting Environment** section below.)

## This is a Quarto Document

Quarto enables you to weave together content and executable code into a finished document. To learn more about Quarto see <https://quarto.org>.


# 1 Setting Up Environment

```{r define constant}
USEGOODDATA <- TRUE
```

```{r load required packages}
# we prefer to use pacman to manage cran packages. 
# First check if pacman is installed. If not, install it.
if(!"pacman" %in% installed.packages()) {
  install.packages("pacman")
}

# load and install packages using pacman. 
# Pacman will install and load missing packages.
pacman::p_load("here") #used to simplify calls to local directory

```

## Secrets and tokens

In order to pull data directly from REDCap, API information must be provided. You should create a file called "secrets.txt" in this directory. This file should contain the following information:

Note, that the .gitignore for this project is configured to exclude your secrets.txt file, so, by default, it will not upload to github, and you will not be able to see other users' secrets.txt. They are only stored on your machine.


# 2 Load Data

```{r import data}
library(RCurl)
library(jsonlite)
library(here)
library(data.table)
if (file.exists(here("2023/secrets.txt"))) {
  source(here("2023/secrets.txt"))
} else {
  warning("secrets not found. checking for data.csv")
  if(!file.exists(here("2023/data.csv"))){
    stop("data not found")
  }
}


#!/usr/bin/env Rscript
token <- "FBB8FA4EA7136C81F88BEA9D15254338"
url <- "https://redcap.iths.org/api/"

formData <- list("token"=token,
    content='record',
    action='export',
    format='json',
    type='flat',
    csvDelimiter='',
    rawOrLabel='label',
    rawOrLabelHeaders='label',
    exportCheckboxLabel='true',
    exportSurveyFields='true',
    exportDataAccessGroups='false',
    returnFormat='json',
    dateRangeBegin='2023-07-01 01:01:01'
)
response <- httr::POST(url, body = formData, encode = "form")
result <- httr::content(response)
#print(result)
```

```{r}
result <- postForm(
    api_url,
    token=api_token,
    content='record',
    format='json',
    type='flat',
    csvDelimiter='',
    rawOrLabel='label',
    rawOrLabelHeaders='label',
    exportCheckboxLabel='true',
    exportSurveyFields='true',
    exportDataAccessGroups='false',
    returnFormat='json',
    dateRangeBegin='2023-07-01 01:01:01'
)

AllSOPARCObservations <- as.data.table(fromJSON(result), )

```

```{r preliminary cleaning and data typing}

SOPARCObservations <- copy(AllSOPARCObservations)

#remove pre-study observation (test data)
SOPARCObservations <- SOPARCObservations[record_id >= 141,]


#drop observations without a time, we cannot use these
SOPARCObservations <- SOPARCObservations[start_time != "",]
#drop observations with missing park names, we cannot use these
SOPARCObservations <- SOPARCObservations[!(park_name %in% ""),]

#convert time to time data type and save as seperate variable
SOPARCObservations[, timestampPOSIX := as.POSIXct(strptime(start_time, "%Y-%m-%d %H:%M"), "America/Los_Angeles") ]

#add day of the week
SOPARCObservations[, day := weekdays(timestampPOSIX)]
#add weekend indicator
SOPARCObservations[, weekend := ifelse(day %in% c("Saturday", "Sunday"), 1, 0)]

#convert countables to numeric

#SOPARCObservations$tar_area <- as.numeric(SOPARCObservations$tar_area)
#


```

```{r temp create park and area reference table with quality checks}
referenceDT <- SOPARCObservations[, .(areaCount = max(as.numeric(tar_area))), by = park_name]
observationsPerPark <- SOPARCObservations[, .(observationCount = .N), by = park_name]
parkStats <- merge(referenceDT, observationsPerPark)
parkStats[, expectedObservationCount := areaCount*24]
parkStats[, meetsExpected := ifelse(expectedObservationCount == observationCount, "PASS", "FAIL")]


```

```{r select analysis set to use}
if(USEGOODDATA) {
  SOPARCAnalysisSet <- copy(SOPARCObservations[park_name %in% parkStats[meetsExpected == "PASS"]$park_name]) #only use parks that have expected number of observation
} else {
  SOPARCAnalysisSet <- copy(SOPARCObservations) #use all parks
}
```



# Parameterizing data

Parks are observed multiple times. 8 times per day. These are in a morning, lunch, afternoon, and evening period (two data collections per period). This process is repeated for 3 days (24 sets of observations per park).

Each park is divided into one or more areas. All areas should be observed during a single observation period. Some areas may be further divided into sub areas. In our data, each area is simply assigned a number.

```{r period assignment based on sequence}


  


expectedTimeBreakpoints <- c(0800, #start morning2
                             0830, #between M2 and L1
                             1230, #start L1
                             1300, #start L2
                             1330, # between L2 and A1
                             1530, #start A1
                             1600, #start A2
                             1630, #between A2 and E1
                             1830, #start E1
                             1900) #start E2


breakPointLabels <- c("morning1",
             "morning2",
             "betweenML",
             "lunch1",
             "lunch2",
             "betweenLA",
             "afternoon1",
             "afternoon2",
             "betweeenAE",
             "evening1",
             "evening2")


 
#for remaining observations, create a list of their times, standardized to POSIX
dataCollectionTimes <- as.POSIXct(strptime(SOPARCAnalysisSet$start_time, "%Y-%m-%d %H:%M"), "America/Los_Angeles") 

breakpointAssignments <- breakPointLabels[findInterval(format(dataCollectionTimes, format = "%H%M"), expectedTimeBreakpoints, left.open = T, all.inside = FALSE, rightmost.closed = FALSE)+1]

#assign observed times to scheduled periods. Use only the house (otherwise would attribute date.)
SOPARCAnalysisSet$periodBasedOnBreakpoint <- breakpointAssignments

```

At this point rudementary timestamps have been assigned based on the expected times of observation.
Now we apply a series of heuristics to correct obvious errors.


```{r adjust timestamp assigments}

```


A second approach to assigning time periods relies on the assumption that timestamps are in order (e.g. the first observation of park A area 1 is always "morning1", the second, "morning2" and so on.)
```{r assign time periods based on sequence}

sequenceLabels <- c("morning1",
             "morning2",
             "lunch1",
             "lunch2",
             "afternoon1",
             "afternoon2",
             "evening1",
             "evening2")


SOPARCAnalysisSet[order(park_scan_data_collection_timestamp, tar_area), periodBasedOnSequence := rep_len(sequenceLabels, .N), by = "park_name"]
```





save these data to review timestamp assignments
```{r}
#save for manual inspection
write.csv(SOPARCAnalysisSet, "inspectTimestamps.csv")

```


For each park, there should be 3 days of observation. We will detect this by extracting the date per park and putting observations in order of day "1" "2" "3"
```{r  assign observation day}

# #library(lubridate)
#   for(park in unique(SOPARCObservations$park_name)) {
#     daysOfYear <- yday(SOPARCObservations[park_name %in% park, start_time])
#     dayOfYear <- unique(daysOfYear)
#     if(length(dayOfYear) > 3) { warning("more than three days detected for:", park ,"\n") }
#     if(length(dayOfYear) < 3) { warning("less than three days detected for:", park ,"\n") }
#     for(dayCounter  in 1:length(dayOfYear)) {
#       SOPARCObservations[park_name %in% park & yday(start_time) == dayOfYear[dayCounter], analysisDay := dayCounter]
#     }
#   }
```


```{r prep for analysis QA}

PreQADT <- copy(SOPARCAnalysisSet)

#check if all expected time periods exists and are assigned
for(park in unique(PreQADT$park_name)) {
  multiple <- referenceDT[park_name %in% park,]$areaCount
  #check sequence based times
  for(period in sequenceLabels) {
    if(nrow(PreQADT[park_name %in% park & periodBasedOnSequence %in% period, ]) != multiple*3) { 
      PreQADT[park_name %in% park & periodBasedOnSequence %in% period, PeriodBasedOnSequencePass := 0]
    } else {
      PreQADT[park_name %in% park & periodBasedOnSequence %in% period, PeriodBasedOnSequencePass := 1]
    }
    PreQADT[park_name %in% park & periodBasedOnSequence %in% period, ]
  }
  
  
}

# 
# #check if any time periods are in between assigned periods
# liminalTimes <- c("betweenML",
#                   "betweenLA",
#                   "betweeenAE")
# if(nrow(PreQADT[periodBasedOnBreakpoint %in% liminalTimes,]) >0){
#   propMissing <- round(nrow(PreQADT[periodBasedOnBreakpoint %in% liminalTimes,])/nrow(PreQADT) ,2)*100
#   warning(paste0("%", propMissing, " of observations are in liminal periods and need to be corrected for proper analysis,"))
#   warning("trimming data to only include observations with properly assigned observation periods. Do not use these results in final analysis.")
#   PreQADT[, passQABP := ifelse(!(periodBasedOnBreakpoint %in% liminalTimes)]
# } else {
#   message("OK status: All observations have been assigned to a time period")
# }
# 
# #check if parks have all expected observations
# pass <- 1
# for(park in unique(PreQADT$park_name)) {
# 
# }

PostQADT <- copy(PreQADT)
```


```{r analysis functions}


```

For all analyses, we do not want to distinguish between the first and second of a period (e.g. morning1 and morning2). This chunk attempts to aggregate these according to the following:


```{r }
SOPARCtoAggregate <- copy(PostQADT)



```
